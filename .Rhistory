}
do_PostProcess_FreqTable(dt_ft_3g)
do_PostProcess_FreqTable <- function(dt_ft) {
### Purpose: Given a data.table object, post-process it to reduce the size of the datatable.
### Input: A data.table object.
### Output: A data.table object.
# Remove all entries where frequencies == 1. Should eliminate a lot of noise, and also the size of the data table.
dt_ft <- filter(dt_ft, freq <> 1)
return(dt_ft)
}
do_PostProcess_FreqTable <- function(dt_ft) {
### Purpose: Given a data.table object, post-process it to reduce the size of the datatable.
### Input: A data.table object.
### Output: A data.table object.
# Remove all entries where frequencies == 1. Should eliminate a lot of noise, and also the size of the data table.
dt_ft <- filter(dt_ft, freq != 1)
return(dt_ft)
}
dt_ft_3g <- do_PostProcess_FreqTable(dt_ft_3g)
nrow(dt_ft_3g)
rm(mtcars)
plot_3g <- print_Barchart(head(dt_ft_3g, 10), "Top 10 3-grams by Frequency")
plot_3g
library("stylo")
library("tm")
library("data.table")
library("ggplot2")
library("dplyr")
source("R/functions.R")
vc_sample_filenames <- generate_Samples(percent = 0.2)
system.time(dt_ft_1g <- get_FreqTable_FromFiles(vc_sample_filenames, ngram = 1))
View(dt_ft_1g)
system.time(dt_ft_2g <- get_FreqTable_FromFiles(vc_sample_filenames, ngram = 2))
1564/60
save(list = grep("^dt_ft", ls(), value = T), file = "vars_Task3.RData")
tables()
source("R/functions.R")
system.time(dt_ft_3g <- get_FreqTable_FromFiles(vc_sample_filenames, ngram = 3))
View(dt_ft_3g)
vc_sample_filenames
source("R/functions.R")
system.time(dt_ft_3g <- get_FreqTable_FromFiles(vc_sample_filenames, ngram = 3))
650/60
save(list = grep("^dt_ft", ls(), value = T), file = "vars_Task3.RData")
View(dt_ft_3g)
grep("me the", dt_ft_3g$ngram,value = F)
grep("^me the", dt_ft_3g$ngram,value = F)
dt_ft_3g$ngram[grep("^me the", dt_ft_3g$ngram, value = F)]
dt_ft_3g$ngram[grep("^me the", dt_ft_3g$ngram, value = F),]
dt_ft_3g[grep("^me the", dt_ft_3g$ngram, value = F),]
View(dt_ft_3g[grep("^me the", dt_ft_3g$ngram, value = F),])
View(dt_ft_3g[grep("^but the\\b", dt_ft_3g$ngram, value = F),])
grep("on your toes", dt_ft_3g$ngram, value = T)
grep("on.toes", dt_ft_3g$ngram, value = T)
grep("on[.]toes", dt_ft_3g$ngram, value = T)
grep("on[.]+toes", dt_ft_3g$ngram, value = T)
grep("\\bon\W+(?:\w+\W+){1,6}?toes\\b", dt_ft_3g$ngram, value = T)
grep("\\bon\\W+(?:\\w+\\W+){1,6}?toes\\b", dt_ft_3g$ngram, value = T)
grep("\\blittle\\W+(?:\\w+\\W+){1,6}?fingers\\b", dt_ft_3g$ngram, value = T)
grep("\\bpush\\W+(?:\\w+\\W+){1,6}?fingers\\b", dt_ft_3g$ngram, value = T)
View(dt_ft_3g)
grep("^but the", dt_ft_3g$ngram, value = T)
grep("^but the crowd", dt_ft_3g$ngram, value = T)
grep("^but the player", dt_ft_3g$ngram, value = T)
grep("^but the referee", dt_ft_3g$ngram, value = T)
grep("^but the defense", dt_ft_3g$ngram, value = T)
grep("^but\w+defense", dt_ft_3g$ngram, value = T)
grep("^but\\w+defense", dt_ft_3g$ngram, value = T)
grep("^but\\w+crowd", dt_ft_3g$ngram, value = T)
grep("^but\\w+", dt_ft_3g$ngram, value = T)
grep("^but\\b\\w+", dt_ft_3g$ngram, value = T)
grep("^but \\w+", dt_ft_3g$ngram, value = T)
grep("offense", dt_ft_3g$ngram, value = T)
grep("cute", dt_ft_3g$ngram, value = T)
View(dt_ft_3g[grep("cute", dt_ft_3g$ngram, value = F),])
tables()
# Load libraries and perform other overheads.
library("stylo")
library("tm")
library("data.table")
library("ggplot2")
library("dplyr")
source("R/functions.R")
tables()
dt_ft_4g <- get_FreqTable_FromFiles(vc_sample_filenames, ngram = 4)
?reorder
gc()
tables()
word2pair <- function(words){
sapply(1:(length(words)-1),function(i){paste(words[i],words[i+1])})
}
t <- "reorder is a generic function. The "default" method treats its first argument as a categorical variable, and reorders its levels based on the values of a second variable, usually numeric"
t <- "reorder is a generic function. The default method treats its first argument as a categorical variable, and reorders its levels based on the values of a second variable, usually numeric"
t
word2pair(t)
alphabet()
alphabet
c(a:z)
t <- c("a","b", "c", "d", "e")
word2pair(t)
1000/9
9/1000
9000/1000000
object.size(dt_ft_3g)
class(object.size(dt_ft_3g))
object.size(dt_ft_3g)[1]
object.size(dt_ft_3g)[1] / 1024
object.size(dt_ft_3g)[1] / 1024^2
head(dt_ft_1g, 50)
sum(dt_ft_1g$freq)
dt_ft_1g[1]
dt_ft_1g[1,]
dt_ft_1g[1, 2]
dt_ft_1g[1, 2] / sum(dt_ft_1g$freq)
dt_ft_1g[2, 2] / sum(dt_ft_1g$freq)
dt_ft_1g[1, 2] / sum(dt_ft_1g$freq) / dt_ft_1g[2, 2] / sum(dt_ft_1g$freq)
dt_ft_1g[1, 2] / sum(dt_ft_1g$freq) / (dt_ft_1g[2, 2] / sum(dt_ft_1g$freq))
(dt_ft_1g[1, 2] / sum(dt_ft_1g$freq) ) / (dt_ft_1g[2, 2] / sum(dt_ft_1g$freq))
(dt_ft_1g[1, 2] / sum(dt_ft_1g$freq) ) / (dt_ft_1g[3, 2] / sum(dt_ft_1g$freq))
(dt_ft_1g[1, 2] / sum(dt_ft_1g$freq) ) / (dt_ft_1g[4, 2] / sum(dt_ft_1g$freq))
View(head(dt_ft_1g, 100))
sum(dt_ft_1g$freq)
(dt_ft_1g[1, 2] / 100000000 ) / (dt_ft_1g[4, 2] / 100000000)
(dt_ft_1g[1, 2] / 100000000 ) / (dt_ft_1g[2, 2] / 100000000)
load("C:/Dropbox/Selegie/Coursera - Data Science Specialization/10 Data Science Capstone/project/vars_Task3.RData")
library("stylo")
library("tm")
library("data.table")
library("ggplot2")
library("dplyr")
source("R/functions.R")
vc_sample_filenames <- generate_Samples(percent = 0.2)
getwd()
generate_Samples(percent = 1)
load(vars.RData)
load("vars.RData")
View(dt_ft_4g)
cft_1 <- get_Cum_Freq(dt_ft_1g)
View(cft_1)
source("R/functions.R")
cft_1 <- get_Cum_Freq(dt_ft_1g)
View(cft_1)
class(cft_1$percent_freq)
cft_1$percent_freq[1]
print_Barchart(dt_ft_1g)
library("stylo")
library("tm")
library("data.table")
library("ggplot2")
library("dplyr")
print_Barchart(dt_ft_1g)
print_Barchart(head(dt_ft_1g,20))
print_Barchart(head(dt_ft_1g,50))
plot(x = cft_1$per_cum_freq, y = (as.integer(rownames(cft_1))), type="l", xaxp = c(0, 1, 100), xlab = "Cumulative Percentage Frequency of All Word Instances", ylab = "Number of Unique Words in Freq Table")
object.size(dt_ft_1g)
object.size(dt_ft_1g)[1]
object.size(dt_ft_1g)[1] / 1024^2
object.size(dt_ft_2g)[1] / 1024^2
object.size(dt_ft_2g)[1] / 1024^2
tables()
install.packages("knitr")
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
install.packages("quantmod")
tables()
cft_2 <- get_Cum_Freq(dt_ft_2g)
View(dt_ft_1g)
grep("mm", dt_ft_1g, value = T)
grep("mm", dt_ft_1g$ngram, value = T)
grep("^mm$", dt_ft_1g$ngram, value = T)
grep("^[^ia]$", dt_ft_1g)
grep("^[^ia]$", dt_ft_1g, value = T)
grep("^.$", dt_ft_1g, value = T)
grep("^.", dt_ft_1g, value = T)
grep("^.$", dt_ft_1g$ngram, value = T)
grep("^RT$", dt_ft_1g$ngram, value = T)
grep("^rt$", dt_ft_1g$ngram, value = T)
test <- c("i am Amos", "This is thistletown", "Meow Meow", "am I not handsome?", "is he the one?")
temp <- c("am", "is")
remove_Words_From_Vector(temp, test)
test <- c("i am Amos", "This is thistletown", "Meow Meow", "am I not handsome?", "is he the one?")
to_remove <- c("am", "is")
remove_Words_From_Vector(to_remove, test)
test
remove_Words_From_Vector(to_remove, test)
grep("^rt$", dt_ft_1g$ngram, value = T)
remove_Words_From_Vector("rt", dt_ft_1g$ngram)
length(remove_Words_From_Vector("rt", dt_ft_1g$ngram))
length(dt_ft_1g$ngram)
length(remove_Words_From_Vector(c("rt"), dt_ft_1g$ngram))
dt_ft_1g$ngram["rt"]
dt_ft_1g$ngram["rt",]
grep("^rt$", dt_ft_1g$ngram, value = T)
grep("^rt$", dt_ft_1g$ngram, value = F)
dt_ft_1g$ngram[94]
dt_ft_1g[94]
test <- c("i am Amos", "This is thistletown", "Meow Meow", "am I not handsome?", "is he the one?")
to_remove <- c("am", "is")
remove_Words_From_Vector(to_remove, test)
test
source("R/functions.R")
dt_ft_1g <- get_FreqTable_FromFiles(vc_sample_filenames, ngram = 1)
shiny::runApp()
grep("^rt$", dt_ft_1g$ngram, value = F)
content <- c("rt This is a ReTweet", "after im done", "go to th")
str <- "\\bim|rt|th\\b"
str
grep(str, content)
grep(str, content, value = T)
gsub(str, "X", content)
vc_remove <- c("im", "rt", "th", "re", "pm", "ve", "id", "st", "ur", "ya", "ll", "tv", "la", "mr", "ha", "nd", "co", "ah", "vs", "de", "dr", "yo", "dc", "em", "ff", "dm", "rd", "uk", "da", "ny", "fb", "bc", "al", "mt", "ad", "ms", "xd", "aw", "ex", "ed", "cd", "um", "yr", "cc", "ps", "uh", "dj", "hm", "ie", "el", "jr", "ma", "ii", "oo", "hr", "mi", "ca", "pr", "oz", "sf", "un", "mo", "pa", "ft", "ue", "mm", "ty", "xo", "xx", "sc", "ep", "yu", "jk", "tx", "er", "et", "gm", "wi", "af", "np", "ap", "le", "va", "nc", "eu", "bo", "eh", "fl", "ol", "pc", "os", "pp", "sd", "bs", "ye", "en", "ch", "wo", "mp", "nj", "lo", "se", "ga", "hs", "ne", "bf", "lb", "il", "eg", "nw", "dt", "fi", "bb", "cm", "qb", "fa", "na", "cs", "uc", "iv", "wa", "pg", "sb", "ac", "dd", "op", "sa", "ip", "pt", "az", "jo", "kc", "gr", "sm", "ua", "ab", "ho", "ot", "md", "su", "ta", "fm", "tl", "du", "gf", "hd", "ds", "ky", "dh", "ew", "ss", "gb", "rb", "bn", "km", "rm", "mb", "ct", "ml", "ba", "tn", "hp", "bi", "wk", "bp", "td", "sr", "te", "lp", "di", "kg", "nm", "uu", "vp", "mg", "sp", "aa", "mn", "ww", "au", "qa", "dp", "mj", "aj", "uw", "wr", "pi", "tf", "tt", "og", "mc", "fr", "gt", "hw", "ks", "jb", "vi", "bk", "fo", "ub", "rs", "ut", "po", "qr", "lt", "rv", "cf", "mf", "es", "xp", "iu", "js", "mw", "ag", "ar", "ik", "bg", "bt", "cb", "dl", "sq", "ts", "jp", "px", "ui", "ck", "jj", "si", "sw", "cj", "tj", "li", "cp", "fe", "jc", "ux", "ra", "fu", "oc", "pe", "rr", "ai", "sl", "rg", "wu", "bj", "cr", "br", "cu", "av", "ce", "db", "gs", "hq", "ku", "nl", "wv", "ay", "gi", "gd", "nh", "oj", "pb", "bw", "ge", "lg", "nt", "pu", "cv", "ec", "ri", "ht", "qs", "cl", "fx", "ak", "mu", "pd", "hb", "ja", "sh", "ea", "nb", "nf", "pj", "ws", "jt", "kk", "hh", "ia", "ib", "rn", "ti", "bd", "gc", "ls", "ou", "tb", "ul", "ee", "fn", "iq", "nz", "wb", "hv", "nu", "ob", "oy", "pk", "ni", "xi", "fk", "iz", "jd", "jw", "ow", "xs", "ae", "tm", "uf", "bu", "ju", "tu", "bh", "cw", "tr", "uv", "wt", "yg", "eb", "gl", "qt", "zs", "fc", "gp", "kd", "lv", "mk", "cn", "gw", "je", "ko", "ud", "vh", "dw", "nv", "cg", "gg", "pf", "tk", "ao", "dx", "gn", "pl", "rp", "ru", "tc", "vt", "wc", "bv", "ka", "om", "rj", "sj", "xu", "io", "ir", "ji", "lc", "lj", "ph", "rk", "sk", "tp", "wh", "wp", "ax", "hc", "lm", "sg", "yd", "bm", "fs", "ro", "xl", "ze", "ev", "fd", "mh", "oa", "vu", "vw", "wd", "xm", "gh", "jv", "kb", "ki", "vc", "ng", "nn", "rc", "ej", "fg", "fw", "uo", "dg", "fy", "lr", "bl", "fp", "ic", "ln", "lu", "tw", "vo", "dk", "ef", "ek", "hg", "kt", "mv", "ez", "ix", "kl", "od", "ox", "qc", "yw", "gq", "hu", "jm", "jn", "kp", "ov", "vv", "yi", "jl", "lf", "oi", "bx", "ci", "df", "jh", "pw", "cx", "cy", "dq", "ey", "hk", "hn", "ke", "ld", "ly", "ns", "rw", "sn", "tg", "vb", "dv", "gk", "gu", "qu", "vm", "wm", "xc", "yt", "za", "hl", "ig", "kj", "pv", "rl", "ry", "wx", "ei", "hf", "rf", "vg", "eo", "kw", "nr", "wg", "xv", "yk", "cq", "jg", "lw", "qe", "rh", "rx", "vd", "fh", "lk", "oe", "py", "ug", "zz", "dn", "hy", "ij", "mx", "nk", "qi", "sx", "wf", "zo", "eq", "kr", "lh", "vy", "xb", "zi", "fj", "gj", "hj", "jf", "sv", "xy", "ys", "zp", "dy", "gz", "kv", "lq", "qq", "sy", "sz", "vr", "wl", "aq", "bq", "cz", "dz", "fz", "hz", "pq", "tz", "vj", "wj", "xr", "yh", "yj", "yp", "gv", "ih", "iw", "kf", "kh", "kn", "lx", "qo", "vl", "wn", "wy", "fv", "iy", "jq", "jz", "lz", "nq", "nx", "pn", "qd", "qm", "wz", "xj", "xt", "yl", "ym", "yn", "zh", "jy", "kz", "mz", "pz", "qx", "rz", "uy", "uz", "vk", "vn", "xa", "yb", "yc", "yy", "zg", "zn", "zr")
vc_remove <- c("im", "rt", "th")
vc_remove
vc_to_remove <- vc_remove
vc_to_remove
content <- c("rt This is a ReTweet", "after im done", "go to th")
str <- "\\bim|rt|th\\b"
gsub(str, "X", content)
content <- c("rt This is a ReTweet rt", "after im done", "go to th")
str <- "\\bim|rt|th\\b"
gsub(str, "X", content)
content <- c("rt This is a ReTweet wrtt", "after im done", "go to th")
str <- "\\bim|rt|th\\b"
gsub(str, "X", content)
content <- c("rt This is a ReTweet wrtt", "after im done", "go to th")
str <- "\\b[im|rt|th]\\b"
gsub(str, "X", content)
content <- c("rt This is a ReTweet wrtt", "after im done", "go to th")
str <- "\\b[im|rt|th]{1}\\b"
gsub(str, "X", content)
content <- c("rt This is a ReTweet wrtt", "after im done", "go to th")
str <- "(\W|^)(im|rt|th)(\W|$)"
gsub(str, "X", content)
content <- c("rt This is a ReTweet wrtt", "after im done", "go to th")
str <- "(\W|^)(im|rt|th)(\W|$)"
content <- c("rt This is a ReTweet wrtt", "after im done", "go to th")
str <- "(\\W|^)(im|rt|th)(\\W|$)"
gsub(str, "X", content)
content <- c("rt This is a ReTweet wrtt", "after im done", "go to th")
str <- "(\\b|^)(im|rt|th)(\\b|$)"
gsub(str, "X", content)
x <- "hello!"
x
x[-1]
x[,-1]
-x
x
length(x)
nchar(x)
substr(x, 1, char(x)-1)
substr(x, 1, nchar(x)-1)
str <- "(\\b|^)("
str
vc_to_remove
for(word in vc_to_remove) {
#vc_data <- gsub(paste("\\b", word, "\\b", sep = ""), " ", vc_data)
str <- paste(str, word, "|", sep = "")
}
str
str <- substr(str, 1, nchar(str) - 1)
str
str <- paste(str, ")(\\b|$)", sep = "")
str
# Load libraries and perform other overheads.
library("stylo")
library("tm")
library("data.table")
library("ggplot2")
library("dplyr")
source("R/functions.R")
dt_ft_2g <- get_FreqTable_FromFiles(vc_sample_filenames, ngram = 2)
grep("^rt$", dt_ft_1g$ngram, value = F)
grep("^rt$", dt_ft_2g$ngram, value = F)
grep("^am$", dt_ft_2g$ngram, value = F)
grep("^am$", dt_ft_1g$ngram, value = F)
grep("^i am$", dt_ft_1g$ngram, value = F)
grep("^i am$", dt_ft_2g$ngram, value = F)
grep("^i am$", dt_ft_2g$ngram, value = T)
View(dt_ft_2g)
dt_ft_3g <- get_FreqTable_FromFiles(vc_sample_filenames, ngram = 3)
dt_ft_3g <- get_FreqTable_FromFiles(vc_sample_filenames, ngram = 3)
View(dt_ft_3g)
View(dt_ft_1g)
table()
shiny::runApp()
library("stylo")
library("tm")
library("data.table")
library("ggplot2")
library("dplyr")
source("R/functions.R")
shiny::runApp()
save(list = grep("^dt_ft", ls(), value = T), file = "vars.RData")
load("vars_dt.RData")
cft_1 <- get_Cum_Freq(dt_ft_1g)
cft_2 <- get_Cum_Freq(dt_ft_2g)
cft_3 <- get_Cum_Freq(dt_ft_3g)
cft_4 <- get_Cum_Freq(dt_ft_4g)
rm(list = grep("^dt_ft", ls(), value = T))
save.image("C:/Dropbox/Selegie/Coursera - Data Science Specialization/10 Data Science Capstone/project/.RData")
library("stylo")
library("tm")
library("data.table")
library("ggplot2")
library("dplyr")
source("R/functions.R")
View(tail(cft_1[cft_1$freq > 10, ], 1000))
library("stylo")
library("tm")
library("data.table")
library("ggplot2")
library("dplyr")
source("R/functions.R")
install.packages("manipulate")
dt_cft1 <- shrink_CFT(cft_1, 0.99)  # Consider if need so many entries? We're predicting only 3-5 words!
dt_cft2 <- shrink_CFT(cft_2, 0.90)
dt_cft3 <- shrink_CFT(cft_3, 0.80)
dt_cft4 <- shrink_CFT(cft_4, 0.66)
tables()
?grep
View(dt_cft4)
save.image("C:/Dropbox/Selegie/Coursera - Data Science Specialization/10 Data Science Capstone/project/.RData")
ls()
grep("^(cft|dt_cft)", ls())
grep("^(cft|dt_cft)", ls(), value = T)
save(list = grep("^dt_ft", ls(), value = T), file = "vars_cft.RData")
View(dt_cft4)
grepl("^i am going", dt_cft4)
grep("^i am going", dt_cft4, value = T)
View(dt_cft4)
grep("^i am going to", dt_cft4, value = T)
grep("^i am going to", dt_cft4$ngram, value = T)
grep("^a long walk", dt_cft4$ngram, value = T)
vc_words_3
vc_words_3 <- c("a","long","walk")
vc_words_3
paste(vc_words_3, collapse = ' ')
paste("^", vc_words_3, collapse = ' ')
paste("^", paste(vc_words_3, collapse = ' '), sep = "")
grep(paste("^", paste(vc_words_3, collapse = ' '), sep = ""), dt_cft4$ngram, value = T)
grep(paste("^", paste(vc_words_3, collapse = ' '), sep = ""), dt_cft4$ngram, value = F)
dt_cft4$ngram[138809,]
dt_cft4[138809,]
num_gram <- 4
dt_name <- paste("dt_cft", num_gram, "$gram", sep = "")
dt_name
grep(paste("^", paste(vc_words_3, collapse = ' '), sep = ""), dt_name, value = F)
grep("^a long walk", dt_cft4$ngram, value = T)
grep("^a long walk", dt_cft4$ngram, value = F)
dt_cft4[grep("^a long walk", dt_cft4$ngram, value = F),]
class(dt_cft4[grep("^a long walk", dt_cft4$ngram, value = F),])
get_Words <- function(vc_input) {
vc_words <- txt.to.words(vc_input)  # Need to tokenize to get word count. Alt technique is to use -> sapply(strsplit(str1, " "), length)
num_words <- length(vc_words)  # Get number of words in tokenized user input string.
if (num_words > 3) num_words = 3  # Because we cannot handle more than 3 words, as we only have until 4-gram lookup table.
num_gram <- num_words + 1  # Always use the n-gram table that is 1 gram more than the word count in the user input string.
dt_name <- paste("dt_cft", num_gram, sep = "")
dt_name_ngram <- paste(dt_name, "$ngram", sep = "")
vc_words <- tail(vc_words, num_words)  # Take just the last (num_words) words.
dt <- dt_name[grep(paste("^", paste(vc_words_3, collapse = ' '), sep = ""), dt_name_ngram, value = F), ]  # Get the entries that start with the user input words.
}
get_Words <- function(vc_input) {
vc_words <- txt.to.words(vc_input)  # Need to tokenize to get word count. Alt technique is to use -> sapply(strsplit(str1, " "), length)
num_words <- length(vc_words)  # Get number of words in tokenized user input string.
if (num_words > 3) num_words = 3  # Because we cannot handle more than 3 words, as we only have until 4-gram lookup table.
num_gram <- num_words + 1  # Always use the n-gram table that is 1 gram more than the word count in the user input string.
dt_name <- paste("dt_cft", num_gram, sep = "")
dt_name_ngram <- paste(dt_name, "$ngram", sep = "")
vc_words <- tail(vc_words, num_words)  # Take just the last (num_words) words.
dt <- dt_name[grep(paste("^", paste(vc_words_3, collapse = ' '), sep = ""), dt_name_ngram, value = F), ]  # Get the entries that start with the user input words.
}
get_Words("a long walk")
get_Words <- function(vc_input) {
vc_words <- txt.to.words(vc_input)  # Need to tokenize to get word count. Alt technique is to use -> sapply(strsplit(str1, " "), length)
num_words <- length(vc_words)  # Get number of words in tokenized user input string.
if (num_words > 3) num_words = 3  # Because we cannot handle more than 3 words, as we only have until 4-gram lookup table.
num_gram <- num_words + 1  # Always use the n-gram table that is 1 gram more than the word count in the user input string.
dt_name <- paste("dt_cft", num_gram, sep = "")
dt_name_ngram <- paste(dt_name, "$ngram", sep = "")
vc_words <- tail(vc_words, num_words)  # Take just the last (num_words) words.
dt <- dt_name[grep(paste("^", paste(vc_words, collapse = ' '), sep = ""), dt_name_ngram, value = F), ]  # Get the entries that start with the user input words.
}
get_Words("a long walk")
get_Words <- function(vc_input) {
vc_words <- txt.to.words(vc_input)  # Need to tokenize to get word count. Alt technique is to use -> sapply(strsplit(str1, " "), length)
num_words <- length(vc_words)  # Get number of words in tokenized user input string.
if (num_words > 3) num_words = 3  # Because we cannot handle more than 3 words, as we only have until 4-gram lookup table.
num_gram <- num_words + 1  # Always use the n-gram table that is 1 gram more than the word count in the user input string.
dt_name <- paste("dt_cft", num_gram, sep = "")
dt_name_ngram <- paste(dt_name, "$ngram", sep = "")
vc_words <- tail(vc_words, num_words)  # Take just the last (num_words) words.
dt <- dt_name[grep(paste("^", paste(vc_words, collapse = ' '), sep = ""), dt_name_ngram, value = F), ]  # Get the entries that start with the user input words.
}
debugonce(get_Words)
get_Words("a long walk")
vc_words
paste("^", paste(vc_words, collapse = ' '), sep = "")
dt_name_ngram
grep(paste("^", paste(vc_words, collapse = ' '), sep = ""), dt_name_ngram, value = F)
grep(paste("^", paste(vc_words, collapse = ' '), sep = ""), dt_cft4$ngram, value = F)
grep(paste("^", paste(vc_words, collapse = ' '), sep = ""), (dt_name_ngram), value = F)
assign(dt_name_ngram, paste("dt_cft", num_gram, "$ngram", sep = ""))
temp <- head(dt_cft4, 10)
temp
assign(temp, var_name))
assign(var_name, temp))
assign(var_name, temp)
assign(temp, var_name)
var_name <- "var1"
assign("var1", temp)
dt_name
dt_name <- paste("dt_cft", num_gram, sep = "")
dt_name
assign("var1", dt_name)
example(list2env)
ls()
View(cft_1)
library("stylo")
library("tm")
library("data.table")
library("ggplot2")
library("dplyr")
source("R/functions.R")
# Test with crafted pre-clean pre-tokenized inputs
test0 <- c("to", "take", "a")  # multiple results!
test1 <- c("on", "a", "positive")  # note
test2 <- c("at", "the", "end")  # of
test3 <- c("do", "not", "want")  # to
View(dt_cft1)
grep("^\\b", dt_cft1$ngram, value = T)
get_Words_From_Inputs("", dt_cft1, 5)
source("R/functions.R")
get_Predicted_Words("on a positive", 5)
get_Predicted_Words("to take a", 5)
tables()
View(dt_cft3)
get_Predicted_Words("i do", 5)
get_Predicted_Words("I do", 5)
debugonce(get_Predicted_Words)
get_Predicted_Words("I do", 5)
get_Predicted_Words("I do", 5)
debugonce(get_Predicted_Words)
get_Predicted_Words("I do", 5)
View(dt_cft2)
get_Predicted_Words("I want to", 5)
get_Predicted_Words("I want to go home!", 5)
get_Predicted_Words("to go home!", 5)
get_Predicted_Words("to go home", 5)
get_Predicted_Words("to go home", 15)
get_Predicted_Words("Amos is very", 15)
get_Predicted_Words("Amos is very", 5)
debugonce(get_Predicted_Words)
get_Predicted_Words("Amos is very", 5)
source("R/functions.R")
debugonce(get_Predicted_Words)
get_Predicted_Words("Amos is very", 5)
length((vc_output))
get_Words_From_Inputs("", dt_cft1, 1)
source("R/functions.R")
get_Predicted_Words("Amos is very", 5)
get_Predicted_Words("I am", 5)
get_Predicted_Words("I am so", 5)
tables()
shiny::runApp('shinyapp_ui')
shiny::runApp('shinyapp_ui')
shiny::runApp()
shiny::runApp()
tables()
getwd()
save(list = grep("^dt_cft", ls(), value = T), file = "vars_final.RData")
getwd()
shiny::runApp('shiny_capstone_project')
shiny::runApp('shiny_capstone_project')
shiny::runApp('shiny_capstone_project')
shiny::runApp('shiny_capstone_project')
get_Predicted_Words("Hello", 5)
get_Predicted_Words("Hello shiny", 5)
shiny::runApp('shiny_capstone_project')
get_Predicted_Words("I don't", 5)
getwd()
source("R/functions.R")
get_Predicted_Words("I don't", 5)
get_Predicted_Words("I can't", 5)
get_Predicted_Words("I haven't", 5)
get_Predicted_Words("I ain't", 5)
get_Predicted_Words("I'm", 5)
get_Predicted_Words("She hasn't", 5)
get_Predicted_Words("1434", 5)
get_Predicted_Words("The", 5)
get_Predicted_Words("The best", 5)
shiny::runApp('shiny_capstone_project')
shiny::runApp('shiny_capstone_project')
test0
test1
head(test1, c(2:3))
test1(-1)
test1(1)
test1[-1]
shiny::runApp('shiny_capstone_project')
shiny::runApp('shiny_capstone_project')
test
test1
paste(test1, collapse = ",")
paste(test1, collapse = ", ")
shiny::runApp('shiny_capstone_project')
shiny::runApp('shiny_capstone_project')
# Load libraries and perform other overheads.
shiny::runApp('shiny_capstone_project')
shiny::runApp('shiny_capstone_project')
